# Workshop Week 9 - Text Analytics
In this workshop you will conduct several text analytics using Python's nltk package and others. We will use Kaggle's movie reviews data set:

* https://www.kaggle.com/c/sentiment-analysis-on-movie-reviews/data
* [train.tsv.zip](train.tsv.zip) (local copy)
* [test.tsv.zip](test.tsv.zip) (local copy)


Some of these exercises are based on this datacamp tutorial:

* [Text Analytics for Beginners using NLTK](https://www.datacamp.com/community/tutorials/text-analytics-beginners-nltk)

## 1. Read data and conduct initial exploration (20 minutes + 10 minutes discussion)

Read the training data. Note that the file uses tab as a separator (not a comma). The file is zipped but pandas' `read_csv` can handle zipped files. Use the following command:
```
>>> data = read_csv('train.tsv.zip', sep='\t')
```
Explore the data and answer the following questions:

1. How many reviews are there in total?
2. Sentiments use a rating from 0 to 4. How many reviews are in each rating?
3. What is the average length (in number of characters) of a review?
4. What is the average length of the reviews in each rating?

## 2. Tokenisation and stop words (15 minutes + 10 minutes discussion)

Use NLTK's list of stop words and NLTK's tokeniser.

To obtain the list of stop words:
```
>>> from nltk.corpus import stopwords
>>> nltk_stopwords = stopwords.words('english')
```

To tokenise a review:
```
>>> import nltk
>>> review = ".... here is the review ..."
>>> words = []
>>> for s in nltk.sent_tokenize(rewiew):
...    words += nltk.word_tokenize(s)
```
Answer the following questions:
1. What is the longest review in number of words?
2. What is the longest review after removing stop words?

## 3. Finding parts of speech (20 minutes + 10 minutes discussion)

To find the parts of speech you can use NLTK:
```
>>> import nltk
>>> sent_tokenised = [['first', 'sentence'], ['the', 'second', 'sentence']]
>>> sent_pos = nltk.pos_tag_sents(sent_tokenised)
```

Answer the following questions:

1. What are the 5 most popular adjectives for each of the 5 review ratings? (hint: NLTK's part of speech tag for adjectives is 'JJ'. See https://www.ling.upenn.edu/courses/Fall_2003/ling001/penn_treebank_pos.html)


# Optional Exercises

## A. Word clouds

Word clouds are quick ways to show the most common terms. 

A simple wordcloud (https://www.datacamp.com/community/tutorials/wordcloud-python):
```
>>> from matplotlib import pyplot as plt
>>> from wordcloud import WordCloud
>>> text = " ... "
>>> wordcloud = WordCloud(stopwords=nltk_stopwords).generate(text)
>>> plt.imshow(wordcloud, interpolation='bilinear')
```

Use word clouds to illustrate the following:
1. What are the most popular non-stop words in each review rating? Display a word cloud for each review rating.
2. What are the most popular adjectives in each review rating? Again, display a word cloud for each review rating.

## B. Named entities
NLTK's interface to extract named entities is rather complex. For this optional exercise you can use SpaCy. It is much easier to use than NLTK but it can take long because SpaCy does more than just finding the entities (https://spacy.io/usage/spacy-101):
```
>>> import spacy
>>> import en_core_web_sm
>>> nlp = en_core_web_sm.load()
>>> sentence = "Amin and Diego are working for Macquarie university since 2000 and 2017."
>>> doc = nlp(sentence)
>>> pprint([(X.text, X.label_) for X in doc.ents])
>>> pprint(doc.ents)
```

Answer the following questions:

1. What is the most popular named entity in each of the 5 review ratings?
2. What is the most popular entity type?

Alternatively, if you want to use NLTK to extract named entities, read these stackoverflow discussion threads:
 *  https://stackoverflow.com/questions/31836058/nltk-named-entity-recognition-to-a-python-list
 * https://stackoverflow.com/questions/24398536/named-entity-recognition-with-regular-expression-nltk

